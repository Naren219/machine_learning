{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "840cc88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6e0879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: forward operations for a convolutional neural network\n",
    "\n",
    "Author: Alejandro Escontrela\n",
    "Version: 1.0\n",
    "Date: June 12th, 2018\n",
    "'''\n",
    "\n",
    "#####################################################\n",
    "################ Forward Operations #################\n",
    "#####################################################\n",
    "\n",
    "\n",
    "def convolution(image, filt, bias, s=1):\n",
    "    '''\n",
    "    Confolves `filt` over `image` using stride `s`\n",
    "    '''\n",
    "    (n_f, n_c_f, f, _) = filt.shape # filter dimensions\n",
    "    n_c, in_dim, _ = image.shape # image dimensions\n",
    "    \n",
    "    out_dim = int((in_dim - f)/s)+1 # calculate output dimensions\n",
    "    \n",
    "    assert n_c == n_c_f, \"Dimensions of filter must match dimensions of input image\"\n",
    "    \n",
    "    out = np.zeros((n_f,out_dim,out_dim))\n",
    "    \n",
    "    # convolve the filter over every part of the image, adding the bias at each step. \n",
    "    for curr_f in range(n_f):\n",
    "        curr_y = out_y = 0\n",
    "        while curr_y + f <= in_dim:\n",
    "            curr_x = out_x = 0\n",
    "            while curr_x + f <= in_dim:\n",
    "                out[curr_f, out_y, out_x] = np.sum(filt[curr_f] * image[:,curr_y:curr_y+f, curr_x:curr_x+f]) + bias[curr_f]\n",
    "                curr_x += s\n",
    "                out_x += 1\n",
    "            curr_y += s\n",
    "            out_y += 1\n",
    "        \n",
    "    return out\n",
    "\n",
    "def maxpool(image, f=2, s=2):\n",
    "    '''\n",
    "    Downsample `image` using kernel size `f` and stride `s`\n",
    "    '''\n",
    "    n_c, h_prev, w_prev = image.shape\n",
    "    \n",
    "    h = int((h_prev - f)/s)+1\n",
    "    w = int((w_prev - f)/s)+1\n",
    "    \n",
    "    downsampled = np.zeros((n_c, h, w))\n",
    "    for i in range(n_c):\n",
    "        # slide maxpool window over each part of the image and assign the max value at each step to the output\n",
    "        curr_y = out_y = 0\n",
    "        while curr_y + f <= h_prev:\n",
    "            curr_x = out_x = 0\n",
    "            while curr_x + f <= w_prev:\n",
    "                downsampled[i, out_y, out_x] = np.max(image[i, curr_y:curr_y+f, curr_x:curr_x+f])\n",
    "                curr_x += s\n",
    "                out_x += 1\n",
    "            curr_y += s\n",
    "            out_y += 1\n",
    "    return downsampled\n",
    "\n",
    "def softmax(X):\n",
    "    return np.exp(X)/np.sum(out)\n",
    "\n",
    "def categoricalCrossEntropy(probs, label):\n",
    "    return -np.sum(label * np.log(probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e58d3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Utility methods for a Convolutional Neural Network\n",
    "\n",
    "Author: Alejandro Escontrela\n",
    "Version: V.1.\n",
    "Date: June 12th, 2018\n",
    "'''\n",
    "#####################################################\n",
    "################## Utility Methods ##################\n",
    "#####################################################\n",
    "        \n",
    "def extract_data(filename, num_images, IMAGE_WIDTH):\n",
    "    '''\n",
    "    Extract images by reading the file bytestream. Reshape the read values into a 3D matrix of dimensions [m, h, w], where m \n",
    "    is the number of training examples.\n",
    "    '''\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(IMAGE_WIDTH * IMAGE_WIDTH * num_images)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(num_images, IMAGE_WIDTH*IMAGE_WIDTH)\n",
    "        return data\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "    '''\n",
    "    Extract label into vector of integer values of dimensions [m, 1], where m is the number of images.\n",
    "    '''\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels\n",
    "\n",
    "def initializeFilter(size, scale = 1.0):\n",
    "    stddev = scale/np.sqrt(np.prod(size))\n",
    "    return np.random.normal(loc = 0, scale = stddev, size = size)\n",
    "\n",
    "def initializeWeight(size):\n",
    "    return np.random.standard_normal(size=size) * 0.01\n",
    "\n",
    "def nanargmax(arr):\n",
    "    idx = np.nanargmax(arr)\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs    \n",
    "\n",
    "def predict(image, f1, f2, w3, w4, b1, b2, b3, b4, conv_s = 1, pool_f = 2, pool_s = 2):\n",
    "    '''\n",
    "    Make predictions with trained filters/weights. \n",
    "    '''\n",
    "    conv1 = convolution(image, f1, b1, conv_s) # convolution operation\n",
    "    conv1[conv1<=0] = 0 #relu activation\n",
    "    \n",
    "    conv2 = convolution(conv1, f2, b2, conv_s) # second convolution operation\n",
    "    conv2[conv2<=0] = 0 # pass through ReLU non-linearity\n",
    "    \n",
    "    pooled = maxpool(conv2, pool_f, pool_s) # maxpooling operation\n",
    "    (nf2, dim2, _) = pooled.shape\n",
    "    fc = pooled.reshape((nf2 * dim2 * dim2, 1)) # flatten pooled layer\n",
    "    \n",
    "    z = w3.dot(fc) + b3 # first dense layer\n",
    "    z[z<=0] = 0 # pass through ReLU non-linearity\n",
    "    \n",
    "    out = w4.dot(z) + b4 # second dense layer\n",
    "    probs = softmax(out) # predict class probabilities with the softmax activation function\n",
    "    \n",
    "    return np.argmax(probs), np.max(probs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7be662b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: backpropagation operations for a convolutional neural network\n",
    "\n",
    "Author: Alejandro Escontrela\n",
    "Version: 1.0\n",
    "Date: June 12th, 2018\n",
    "'''\n",
    "#####################################################\n",
    "############### Backward Operations #################\n",
    "#####################################################\n",
    "        \n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s):\n",
    "    '''\n",
    "    Backpropagation through a convolutional layer. \n",
    "    '''\n",
    "    (n_f, n_c, f, _) = filt.shape\n",
    "    (_, orig_dim, _) = conv_in.shape\n",
    "    ## initialize derivatives\n",
    "    dout = np.zeros(conv_in.shape) \n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dbias = np.zeros((n_f,1))\n",
    "    for curr_f in range(n_f):\n",
    "        # loop through all filters\n",
    "        curr_y = out_y = 0\n",
    "        while curr_y + f <= orig_dim:\n",
    "            curr_x = out_x = 0\n",
    "            while curr_x + f <= orig_dim:\n",
    "                # loss gradient of filter (used to update the filter)\n",
    "                dfilt[curr_f] += dconv_prev[curr_f, out_y, out_x] * conv_in[:, curr_y:curr_y+f, curr_x:curr_x+f]\n",
    "                # loss gradient of the input to the convolution operation (conv1 in the case of this network)\n",
    "                dout[:, curr_y:curr_y+f, curr_x:curr_x+f] += dconv_prev[curr_f, out_y, out_x] * filt[curr_f] \n",
    "                curr_x += s\n",
    "                out_x += 1\n",
    "            curr_y += s\n",
    "            out_y += 1\n",
    "        # loss gradient of the bias\n",
    "        dbias[curr_f] = np.sum(dconv_prev[curr_f])\n",
    "    \n",
    "    return dout, dfilt, dbias\n",
    "\n",
    "\n",
    "\n",
    "def maxpoolBackward(dpool, orig, f, s):\n",
    "    '''\n",
    "    Backpropagation through a maxpooling layer. The gradients are passed through the indices of greatest value in the original maxpooling during the forward step.\n",
    "    '''\n",
    "    (n_c, orig_dim, _) = orig.shape\n",
    "    \n",
    "    dout = np.zeros(orig.shape)\n",
    "    \n",
    "    for curr_c in range(n_c):\n",
    "        curr_y = out_y = 0\n",
    "        while curr_y + f <= orig_dim:\n",
    "            curr_x = out_x = 0\n",
    "            while curr_x + f <= orig_dim:\n",
    "                # obtain index of largest value in input for current window\n",
    "                (a, b) = nanargmax(orig[curr_c, curr_y:curr_y+f, curr_x:curr_x+f])\n",
    "                dout[curr_c, curr_y+a, curr_x+b] = dpool[curr_c, out_y, out_x]\n",
    "                \n",
    "                curr_x += s\n",
    "                out_x += 1\n",
    "            curr_y += s\n",
    "            out_y += 1\n",
    "        \n",
    "    return dout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "202b0b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train-images-idx3-ubyte.gz\n",
      "Extracting train-labels-idx1-ubyte.gz\n",
      "LR:0.01, Batch Size:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-49b974387179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'paramz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7e111d19cc1e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_classes, lr, beta1, beta2, img_dim, img_depth, f, num_filt1, num_filt2, batch_size, num_epochs, save_path)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madamGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cost: %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7e111d19cc1e>\u001b[0m in \u001b[0;36madamGD\u001b[0;34m(batch, num_classes, lr, dim, n_c, beta1, beta2, params, cost)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Collect Gradients for training example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mdf1_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw3_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw4_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb1_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb2_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb3_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb4_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7e111d19cc1e>\u001b[0m in \u001b[0;36mconv\u001b[0;34m(image, label, params, conv_s, pool_f, pool_s)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb4\u001b[0m \u001b[0;31m# second dense layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# predict class probabilities with the softmax activation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-cab3713bf0fe>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcategoricalCrossEntropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Description: Script to train the network and measure its performance on the test set.\n",
    "\n",
    "Author: Alejandro Escontrela\n",
    "Version: V.1.\n",
    "Date: June 12th, 2018\n",
    "'''\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    save_path = 'paramz'\n",
    "    \n",
    "    cost = train(save_path = save_path)\n",
    "\n",
    "    params, cost = pickle.load(open(save_path, 'rb'))\n",
    "    [f1, f2, w3, w4, b1, b2, b3, b4] = params\n",
    "    \n",
    "    # Plot cost \n",
    "    plt.plot(cost, 'r')\n",
    "    plt.xlabel('# Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.legend('Loss', loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # Get test data\n",
    "    m =10000\n",
    "    X = extract_data('t10k-images-idx3-ubyte.gz', m, 28)\n",
    "    y_dash = extract_labels('t10k-labels-idx1-ubyte.gz', m).reshape(m,1)\n",
    "    # Normalize the data\n",
    "    X-= int(np.mean(X)) # subtract mean\n",
    "    X/= int(np.std(X)) # divide by standard deviation\n",
    "    test_data = np.hstack((X,y_dash))\n",
    "    \n",
    "    X = test_data[:,0:-1]\n",
    "    X = X.reshape(len(test_data), 1, 28, 28)\n",
    "    y = test_data[:,-1]\n",
    "\n",
    "    corr = 0\n",
    "    digit_count = [0 for i in range(10)]\n",
    "    digit_correct = [0 for i in range(10)]\n",
    "   \n",
    "    print()\n",
    "    print(\"Computing accuracy over test set:\")\n",
    "\n",
    "    t = tqdm(range(len(X)), leave=True)\n",
    "\n",
    "    for i in t:\n",
    "        x = X[i]\n",
    "        pred, prob = predict(x, f1, f2, w3, w4, b1, b2, b3, b4)\n",
    "        digit_count[int(y[i])]+=1\n",
    "        if pred==y[i]:\n",
    "            corr+=1\n",
    "            digit_correct[pred]+=1\n",
    "\n",
    "        t.set_description(\"Acc:%0.2f%%\" % (float(corr/(i+1))*100))\n",
    "        \n",
    "    print(\"Overall Accuracy: %.2f\" % (float(corr/len(test_data)*100)))\n",
    "    x = np.arange(10)\n",
    "    digit_recall = [x/y for x,y in zip(digit_correct, digit_count)]\n",
    "    plt.xlabel('Digits')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title(\"Recall on Test Set\")\n",
    "    plt.bar(x,digit_recall)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
